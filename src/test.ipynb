{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import CSVLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "def vct_db_filename_gen(file_path):\n",
    "    # Derive vector DB filename from CSV filename\n",
    "    base_name = os.path.basename(file_path)\n",
    "    db_file_name = os.path.splitext(base_name)[0] + \".vecdb\"\n",
    "\n",
    "    return os.path.join(os.path.dirname(file_path), db_file_name)\n",
    "\n",
    "def check_and_load_vector_db(file_path, embedding):\n",
    "    \"\"\"\n",
    "    Checks if a vector db file exists for the given file_path, \n",
    "    loads it if exists, otherwise creates it from the csv and saves it.\n",
    "    \"\"\"\n",
    "    # Derive vector DB filename from CSV filename\n",
    "    db_file_path = vct_db_filename_gen(file_path)\n",
    "\n",
    "    # Check if the vector DB file exists\n",
    "    if os.path.exists(db_file_path):\n",
    "        print(f\"Loading existing vector DB from {db_file_path}\")\n",
    "        db = Chroma(persist_directory=db_file_path, embedding_function=embedding)\n",
    "    else:\n",
    "        print(f\"Vector DB not found. Creating from {file_path}\")\n",
    "        # Load the CSV and create the vector DB\n",
    "        loader = CSVLoader(file_path=file_path)\n",
    "        documents = loader.load()\n",
    "        # Save the newly created vector DB\n",
    "        db = Chroma.from_documents(documents, embedding, persist_directory=db_file_path)\n",
    "        print(f\"Saved new vector DB to {db_file_path}\")\n",
    "    \n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResultsData:\n",
    "    def __init__(self, chain_type, time=None, tokens_used=None, example_number=None, predicted_query=None, predicted_answer=None, answer=None, result=None):\n",
    "        self.chain_type = chain_type\n",
    "        self.eval = []\n",
    "        if example_number is not None:\n",
    "            self.append_evaluation(time, tokens_used, example_number, predicted_query, answer, predicted_answer, result)\n",
    "    \n",
    "    def append_evaluation(self, time, tokens_used, example_number, predicted_query, answer, predicted_answer, result):\n",
    "        \"\"\"Append a new evaluation result to the eval list.\"\"\"\n",
    "        self.eval.append({\n",
    "            \"time\": time,\n",
    "            \"tokens_used\": tokens_used,\n",
    "            \"example_number\": example_number,\n",
    "            \"query\": predicted_query,\n",
    "            \"predicted_answer\": predicted_answer,\n",
    "            \"answer\": answer,\n",
    "            \"result\": result\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "# account for deprecation of LLM model\n",
    "\n",
    "def llm_model():\n",
    "    # Get the current date\n",
    "    current_date = datetime.datetime.now().date()\n",
    "\n",
    "    # Define the date after which the model should be set to \"gpt-3.5-turbo\"\n",
    "    target_date = datetime.date(2024, 6, 12)\n",
    "\n",
    "    # Set the model variable based on the current date\n",
    "    if current_date > target_date:\n",
    "        return \"gpt-3.5-turbo\"\n",
    "    else:\n",
    "        return \"gpt-3.5-turbo-0301\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation.qa import QAGenerateChain\n",
    "from langchain.evaluation.qa import QAEvalChain\n",
    "from langchain.indexes.vectorstore import VectorStoreIndexWrapper\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.document_loaders import CSVLoader\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "def langchain_output_parser(qa_output):\n",
    "    \"\"\"\n",
    "    Transforms the QA output from langchain into a dictionary format without the 'qa_pairs' field.\n",
    "    \n",
    "    Parameters:\n",
    "    - qa_output: A list of dictionaries, where each dictionary contains 'qa_pairs' among other possible fields.\n",
    "\n",
    "    Returns:\n",
    "    - A list of dictionaries, where each dictionary directly contains 'query' and 'answer' fields.\n",
    "    \"\"\"\n",
    "    parsed_output = []\n",
    "    for item in qa_output:\n",
    "        # Assuming each item in qa_output is a dictionary with a 'qa_pairs' key\n",
    "        qa_pair = item.get('qa_pairs', {})\n",
    "        # Repackage the qa_pair without the 'qa_pairs' field\n",
    "        reformatted_item = {\n",
    "            'query': qa_pair.get('query', ''),\n",
    "            'answer': qa_pair.get('answer', '')\n",
    "        }\n",
    "        parsed_output.append(reformatted_item)\n",
    "    return parsed_output\n",
    "\n",
    "\n",
    "def generate_qas(file_path, db, llm, chain_type):\n",
    "    # Load vector db to index\n",
    "    loader = CSVLoader(file_path=file_path)\n",
    "    data = loader.load()\n",
    "    index = VectorStoreIndexWrapper(vectorstore=db)\n",
    "\n",
    "    qa = RetrievalQA.from_chain_type(\n",
    "        llm=llm, \n",
    "        chain_type=chain_type, \n",
    "        retriever=index.vectorstore.as_retriever(), \n",
    "        verbose=True,\n",
    "        chain_type_kwargs = {\n",
    "            \"document_separator\": \"<<<<>>>>>\"\n",
    "        }\n",
    "    ) \n",
    "\n",
    "    # LLM-Generated example Q&A pairs \n",
    "    example_gen_chain = QAGenerateChain.from_llm(ChatOpenAI(model=llm_model()))\n",
    "    # the warning below can be safely ignored\n",
    "    raw_examples = example_gen_chain.apply( # create raw examples\n",
    "        [{\"doc\": t} for t in data[:5]],\n",
    "    )\n",
    "\n",
    "    # Parse the raw examples into required format\n",
    "    examples = langchain_output_parser(raw_examples)\n",
    "\n",
    "    # run for manual evaluation\n",
    "    qa.run(examples[0][\"query\"])\n",
    "\n",
    "    return qa, examples\n",
    "\n",
    "def evaluate(chain_type, qa, examples, llm, results_data):\n",
    "    # LLM assisted evaluation\n",
    "    predictions = qa.apply(examples)\n",
    "    eval_chain = QAEvalChain.from_llm(llm)\n",
    "    graded_outputs = eval_chain.evaluate(examples, predictions)\n",
    "\n",
    "    # turn to object and return\n",
    "    # using llm as real answer and predicted answer are not similar in a string match sense, e.g. look at example_llm_eval.txt\n",
    "    for i, eg in enumerate(examples):\n",
    "        \n",
    "        example_number = i\n",
    "        query = predictions[i]['query']\n",
    "        answer = predictions[i]['answer']\n",
    "        predicted_answer = predictions[i]['result']\n",
    "        result = graded_outputs[i]['results']\n",
    "        \n",
    "        print(f\"Example {example_number}:\")\n",
    "        print(\"Question: \" + query)\n",
    "        print(\"Real Answer: \" + answer)\n",
    "        print(\"Predicted Answer: \" + predicted_answer)\n",
    "        print(\"Predicted Grade: \" + result)\n",
    "        print()\n",
    "\n",
    "        results_data = add_to_results_list(results_data, chain_type, query, answer=answer, result=result)\n",
    "    return results_data\n",
    "\n",
    "def add_to_results_list(results_data, chain_type, query, td=None, tokens_used=None, number=None, response=None, answer=None, result=None):\n",
    "    found = False\n",
    "    for item in results_data:\n",
    "        if item.chain_type == chain_type:\n",
    "            # Update the existing dictionary\n",
    "            item.append_evaluation(time=td, tokens_used=tokens_used, example_number=number, \n",
    "                         predicted_query=query, answer=response, predicted_answer=answer, result=result)\n",
    "            found = True\n",
    "            break\n",
    "\n",
    "    if not found:\n",
    "        # Append a new instance of ResultsData if no matching chain_type was found\n",
    "        results_data.append(ResultsData(chain_type=chain_type, time=td, tokens_used=tokens_used, \n",
    "                                        example_number=number, predicted_query=query, \n",
    "                                        answer=response, predicted_answer=answer, \n",
    "                                        result=result))\n",
    "        \n",
    "    return results_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "def qa_analysis(llm, chain_type, retriever, verbose, query, number, results_data):\n",
    "    \"\"\"\n",
    "    Initializes a QA analysis with a given language model, chain type, and retriever.\n",
    "    Then, it runs the QA analysis, timing its execution and printing the response along with the execution time.\n",
    "    \"\"\"\n",
    "    # Initialize the RetrievalQA object with the specified parameters.\n",
    "    qa = RetrievalQA.from_chain_type(\n",
    "        llm=llm, \n",
    "        chain_type=chain_type, \n",
    "        retriever=retriever, \n",
    "        verbose=verbose\n",
    "    )\n",
    "\n",
    "    # Measure number of tokens used\n",
    "    with get_openai_callback() as cb:\n",
    "        start = datetime.datetime.now()\n",
    "\n",
    "        try:\n",
    "            # Execute the QA analysis\n",
    "            response = qa.invoke(query) #TODO: i've only added queries, no answers...\n",
    "        except ValueError as e: \n",
    "            response = e\n",
    "\n",
    "        end = datetime.datetime.now()\n",
    "    \n",
    "    tokens_used = cb.total_tokens\n",
    "\n",
    "    # Calculate the difference between the end and start timestamps to get the execution duration.\n",
    "    # The duration is converted to milliseconds for a more precise and readable format.\n",
    "    td = (end - start).total_seconds() * 10**3\n",
    "    \n",
    "    print(f\"Response: {response}\\nThe time of execution of above program is : {td:.03f}ms\")\n",
    "\n",
    "    results_data = add_to_results_list(results_data, chain_type, query, td, tokens_used, number, response)\n",
    "\n",
    "    print(\"\\n\\nTESTING\\n:\" + '\\n'.join([str(item) for item in results_data]))\n",
    "\n",
    "    return results_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def results_data_to_markdown_table(results_data_list):\n",
    "    # Define the header of the markdown table\n",
    "    headers = [\"Chain Type\", \"Eval Time\", \"Tokens Used\", \"Example Number\", \"Predicted Query\", \"Predicted Answer\", \"Answer\", \"Result\"]\n",
    "    # Create the markdown table header and separator rows\n",
    "    markdown_table = \"| \" + \" | \".join(headers) + \" |\\n\"\n",
    "    markdown_table += \"| \" + \" | \".join([\"---\"] * len(headers)) + \" |\\n\"\n",
    "    \n",
    "    # Iterate over each ResultsData instance\n",
    "    for data in results_data_list:\n",
    "        # And then iterate over each evaluation within the ResultsData instance\n",
    "        for eval in data.eval:\n",
    "            # Construct each row with the appropriate data\n",
    "            row = [\n",
    "                data.chain_type,  # Corrected from data.type to data.chain_type\n",
    "                str(eval.get(\"time\", \"\")),  # Using .get() for safer access to dictionary keys\n",
    "                str(eval.get(\"tokens_used\", \"\")),\n",
    "                str(eval.get(\"example_number\", \"\")),\n",
    "                eval.get(\"query\", \"\"),\n",
    "                eval.get(\"predicted_answer\", \"\"),\n",
    "                eval.get(\"answer\", \"\"),\n",
    "                eval.get(\"result\", \"\")\n",
    "            ]\n",
    "            markdown_table += \"| \" + \" | \".join(row) + \" |\\n\"\n",
    "    \n",
    "    return markdown_table\n",
    "\n",
    "def write_markdown_table_to_file(markdown_table, filename):\n",
    "    # Write the markdown table to the specified file\n",
    "    with open(filename, 'w', encoding='utf-8') as file:\n",
    "        file.write(markdown_table)\n",
    "    \n",
    "    print(f\"Markdown table successfully written to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing vector DB from ../data/OutdoorClothingCatalog_1000.vecdb\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Example 0:\n",
      "Question: What is the approximate weight of the Women's Campside Oxfords per pair?\n",
      "Real Answer: The approximate weight of the Women's Campside Oxfords per pair is 1 lb.1 oz.\n",
      "Predicted Answer: The approximate weight of the Women's Campside Oxfords per pair is 1 lb. 1 oz.\n",
      "Predicted Grade: CORRECT\n",
      "\n",
      "Example 1:\n",
      "Question: What are the dimensions of the small and medium Recycled Waterhog dog mats?\n",
      "Real Answer: The small Recycled Waterhog dog mat has dimensions of 18\" x 28\", while the medium mat has dimensions of 22.5\" x 34.5\".\n",
      "Predicted Answer: The small Recycled Waterhog dog mat has dimensions of 18\" x 28\" and the medium has dimensions of 22.5\" x 34.5\".\n",
      "Predicted Grade: CORRECT\n",
      "\n",
      "Example 2:\n",
      "Question: What are some features of the Infant and Toddler Girls' Coastal Chill Swimsuit?\n",
      "Real Answer: The swimsuit has bright colors, ruffles and exclusive whimsical prints. The fabric is four-way-stretch and chlorine-resistant, and it has UPF 50+ rated fabric that blocks 98% of the sun's harmful rays. The swimsuit has crossover no-slip straps and is fully lined on the bottom for a secure fit and maximum coverage. It can be machine washed and line dried for best results, and it is imported.\n",
      "Predicted Answer: The Infant and Toddler Girls' Coastal Chill Swimsuit is a two-piece swimsuit with bright colors, ruffles, and exclusive whimsical prints. It is made of four-way-stretch and chlorine-resistant fabric that keeps its shape and resists snags. The swimsuit has UPF 50+ rated fabric that provides the highest rated sun protection possible, blocking 98% of the sun's harmful rays. The crossover no-slip straps and fully lined bottom ensure a secure fit and maximum coverage. It is machine washable and should be line dried for best results.\n",
      "Predicted Grade: CORRECT\n",
      "\n",
      "Example 3:\n",
      "Question: What is the percentage of recycled nylon used in the construction of the Refresh Swimwear, V-Neck Tankini Contrasts?\n",
      "Real Answer: The Refresh Swimwear, V-Neck Tankini Contrasts is made of 82% recycled nylon with 18% Lycra® spandex.\n",
      "Predicted Answer: The Refresh Swimwear, V-Neck Tankini Contrasts is made with 82% recycled nylon and 18% Lycra® spandex.\n",
      "Predicted Grade: CORRECT\n",
      "\n",
      "Example 4:\n",
      "Question: What is the main feature of the EcoFlex 3L Storm Pants that makes them more breathable?\n",
      "Real Answer: The EcoFlex 3L Storm Pants have TEK O2 technology, which offers the most breathability the company has ever tested.\n",
      "Predicted Answer: The main feature of the EcoFlex 3L Storm Pants that makes them more breathable is the state-of-the-art TEK O2 technology.\n",
      "Predicted Grade: CORRECT\n",
      "\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for MapReduceDocumentsChain\ndocument_separator\n  extra fields not permitted (type=value_error.extra)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 49\u001b[0m\n\u001b[1;32m     46\u001b[0m     write_markdown_table_to_file(md_table, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresults.md\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 49\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 35\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Manual analysis - TODO: add answers\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# for index, query in enumerate(queries, start=1):\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m#     results_data = qa_analysis(llm, \"stuff\", retriever, True, query, index, results_data)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m \n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# LLM QA Gen AND Evaluate\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m strat \u001b[38;5;129;01min\u001b[39;00m strategies:\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m# Generate evaluation Q&As\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m     \u001b[38;5;28mtuple\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_qas\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrat\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#TODO: change this?\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     qa \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     37\u001b[0m     examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;241m1\u001b[39m]\n",
      "Cell \u001b[0;32mIn[11], line 37\u001b[0m, in \u001b[0;36mgenerate_qas\u001b[0;34m(file_path, db, llm, chain_type)\u001b[0m\n\u001b[1;32m     34\u001b[0m data \u001b[38;5;241m=\u001b[39m loader\u001b[38;5;241m.\u001b[39mload()\n\u001b[1;32m     35\u001b[0m index \u001b[38;5;241m=\u001b[39m VectorStoreIndexWrapper(vectorstore\u001b[38;5;241m=\u001b[39mdb)\n\u001b[0;32m---> 37\u001b[0m qa \u001b[38;5;241m=\u001b[39m \u001b[43mRetrievalQA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_chain_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchain_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchain_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretriever\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvectorstore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_retriever\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchain_type_kwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdocument_separator\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m<<<<>>>>>\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# LLM-Generated example Q&A pairs \u001b[39;00m\n\u001b[1;32m     48\u001b[0m example_gen_chain \u001b[38;5;241m=\u001b[39m QAGenerateChain\u001b[38;5;241m.\u001b[39mfrom_llm(ChatOpenAI(model\u001b[38;5;241m=\u001b[39mllm_model()))\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/langchain/chains/retrieval_qa/base.py:105\u001b[0m, in \u001b[0;36mBaseRetrievalQA.from_chain_type\u001b[0;34m(cls, llm, chain_type, chain_type_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load chain from chain type.\"\"\"\u001b[39;00m\n\u001b[1;32m    104\u001b[0m _chain_type_kwargs \u001b[38;5;241m=\u001b[39m chain_type_kwargs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[0;32m--> 105\u001b[0m combine_documents_chain \u001b[38;5;241m=\u001b[39m \u001b[43mload_qa_chain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchain_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchain_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_chain_type_kwargs\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(combine_documents_chain\u001b[38;5;241m=\u001b[39mcombine_documents_chain, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/langchain/chains/question_answering/__init__.py:249\u001b[0m, in \u001b[0;36mload_qa_chain\u001b[0;34m(llm, chain_type, verbose, callback_manager, **kwargs)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chain_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m loader_mapping:\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    246\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot unsupported chain type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchain_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    247\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloader_mapping\u001b[38;5;241m.\u001b[39mkeys()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    248\u001b[0m     )\n\u001b[0;32m--> 249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloader_mapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mchain_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/langchain/chains/question_answering/__init__.py:162\u001b[0m, in \u001b[0;36m_load_map_reduce_chain\u001b[0;34m(llm, question_prompt, combine_prompt, combine_document_variable_name, map_reduce_document_variable_name, collapse_prompt, reduce_llm, collapse_llm, verbose, callback_manager, callbacks, token_max, **kwargs)\u001b[0m\n\u001b[1;32m    144\u001b[0m     collapse_chain \u001b[38;5;241m=\u001b[39m StuffDocumentsChain(\n\u001b[1;32m    145\u001b[0m         llm_chain\u001b[38;5;241m=\u001b[39mLLMChain(\n\u001b[1;32m    146\u001b[0m             llm\u001b[38;5;241m=\u001b[39m_collapse_llm,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    154\u001b[0m         callback_manager\u001b[38;5;241m=\u001b[39mcallback_manager,\n\u001b[1;32m    155\u001b[0m     )\n\u001b[1;32m    156\u001b[0m reduce_documents_chain \u001b[38;5;241m=\u001b[39m ReduceDocumentsChain(\n\u001b[1;32m    157\u001b[0m     combine_documents_chain\u001b[38;5;241m=\u001b[39mcombine_documents_chain,\n\u001b[1;32m    158\u001b[0m     collapse_documents_chain\u001b[38;5;241m=\u001b[39mcollapse_chain,\n\u001b[1;32m    159\u001b[0m     token_max\u001b[38;5;241m=\u001b[39mtoken_max,\n\u001b[1;32m    160\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m    161\u001b[0m )\n\u001b[0;32m--> 162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mMapReduceDocumentsChain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm_chain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmap_chain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdocument_variable_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmap_reduce_document_variable_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreduce_documents_chain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreduce_documents_chain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallback_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/langchain_core/load/serializable.py:107\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lc_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pydantic/v1/main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[0;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for MapReduceDocumentsChain\ndocument_separator\n  extra fields not permitted (type=value_error.extra)"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "def main():\n",
    "    # Basic Setup\n",
    "    _ = load_dotenv(find_dotenv()) # read local .env file\n",
    "    results_data = []\n",
    "    strategies = [\"stuff\", \"map_reduce\", \"refine\", \"map_rerank\"]\n",
    "\n",
    "    # Load data into vector db or use existing one\n",
    "    file_path = '../data/OutdoorClothingCatalog_1000.csv'\n",
    "    embedding = OpenAIEmbeddings()  # Define embedding\n",
    "\n",
    "    # Check if vector DB exists for the CSV, and load or create accordingly\n",
    "    db = check_and_load_vector_db(file_path, embedding)\n",
    "\n",
    "    queries = [\"Please suggest a shirt with sunblocking\", \"Please suggest a shirt with sunblocking and tell me why this one\", \"Please suggest three shirts with sunblocking and tell me why. Give this back to me in markdown code as a table\", \"Please suggest three shirts with sunblocking and tell me why. Give this back to me in markdown code as a table, with a summary below outlining why sunblocking is important\"]\n",
    "\n",
    "    # Configure LLM for querying\n",
    "    # layers vector db on llm to inform decisions and responses\n",
    "    llm = ChatOpenAI(temperature = 0.0, model=llm_model())\n",
    "    retriever = db.as_retriever()\n",
    "\n",
    "    # Manual analysis - TODO: add answers\n",
    "    # for index, query in enumerate(queries, start=1):\n",
    "    #     results_data = qa_analysis(llm, \"stuff\", retriever, True, query, index, results_data)\n",
    "    #     results_data = qa_analysis(llm, \"map_reduce\", retriever, True, query, index, results_data)\n",
    "    #     results_data = qa_analysis(llm, \"refine\", retriever, True, query, index, results_data)\n",
    "    #     results_data = qa_analysis(llm, \"map_rerank\", retriever, True, query, index, results_data)\n",
    "\n",
    "    # LLM QA Gen AND Evaluate\n",
    "    for strat in strategies:\n",
    "        # Generate evaluation Q&As\n",
    "        tuple = generate_qas(file_path, db, llm, strat)\n",
    "        qa = tuple[0]\n",
    "        examples = tuple[1]\n",
    "\n",
    "        # Evaluate \n",
    "        results_data = evaluate(strat, qa, examples, llm, results_data)\n",
    "\n",
    "    # Generate results in markdown\n",
    "    md_table = results_data_to_markdown_table(results_data)\n",
    "\n",
    "    # Write results to file\n",
    "    write_markdown_table_to_file(md_table, \"results.md\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
