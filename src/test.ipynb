{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import CSVLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "def vct_db_filename_gen(file_path):\n",
    "    # Derive vector DB filename from CSV filename\n",
    "    base_name = os.path.basename(file_path)\n",
    "    db_file_name = os.path.splitext(base_name)[0] + \".vecdb\"\n",
    "\n",
    "    return os.path.join(os.path.dirname(file_path), db_file_name)\n",
    "\n",
    "def check_and_load_vector_db(file_path, embedding):\n",
    "    \"\"\"\n",
    "    Checks if a vector db file exists for the given file_path, \n",
    "    loads it if exists, otherwise creates it from the csv and saves it.\n",
    "    \"\"\"\n",
    "    # Derive vector DB filename from CSV filename\n",
    "    db_file_path = vct_db_filename_gen(file_path)\n",
    "\n",
    "    # Check if the vector DB file exists\n",
    "    if os.path.exists(db_file_path):\n",
    "        print(f\"Loading existing vector DB from {db_file_path}\")\n",
    "        db = Chroma(persist_directory=db_file_path, embedding_function=embedding)\n",
    "    else:\n",
    "        print(f\"Vector DB not found. Creating from {file_path}\")\n",
    "        # Load the CSV and create the vector DB\n",
    "        loader = CSVLoader(file_path=file_path)\n",
    "        documents = loader.load()\n",
    "        # Save the newly created vector DB\n",
    "        db = Chroma.from_documents(documents, embedding, persist_directory=db_file_path)\n",
    "        print(f\"Saved new vector DB to {db_file_path}\")\n",
    "    \n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResultsData:\n",
    "    def __init__(self, chain_type, time=None, tokens_used=None, example_number=None, predicted_query=None, predicted_answer=None, answer=None, result=None):\n",
    "        self.chain_type = chain_type\n",
    "        self.eval = []\n",
    "        if example_number is not None:\n",
    "            self.append_evaluation(time, tokens_used, example_number, predicted_query, answer, predicted_answer, result)\n",
    "    \n",
    "    def append_evaluation(self, time, tokens_used, example_number, predicted_query, answer, predicted_answer, result):\n",
    "        \"\"\"Append a new evaluation result to the eval list.\"\"\"\n",
    "        self.eval.append({\n",
    "            \"time\": time,\n",
    "            \"tokens_used\": tokens_used,\n",
    "            \"example_number\": example_number,\n",
    "            \"query\": predicted_query,\n",
    "            \"predicted_answer\": predicted_answer,\n",
    "            \"answer\": answer,\n",
    "            \"result\": result\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "# account for deprecation of LLM model\n",
    "\n",
    "def llm_model():\n",
    "    # Get the current date\n",
    "    current_date = datetime.datetime.now().date()\n",
    "\n",
    "    # Define the date after which the model should be set to \"gpt-3.5-turbo\"\n",
    "    target_date = datetime.date(2024, 6, 12)\n",
    "\n",
    "    # Set the model variable based on the current date\n",
    "    if current_date > target_date:\n",
    "        return \"gpt-3.5-turbo\"\n",
    "    else:\n",
    "        return \"gpt-3.5-turbo-0301\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation.qa import QAGenerateChain\n",
    "from langchain.evaluation.qa import QAEvalChain\n",
    "from langchain.indexes.vectorstore import VectorStoreIndexWrapper\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.document_loaders import CSVLoader\n",
    "from modules.set_model import llm_model\n",
    "from langchain_openai import ChatOpenAI\n",
    "from modules.results_data import ResultsData\n",
    "from datetime import datetime\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "def langchain_output_parser(qa_output):\n",
    "    \"\"\"\n",
    "    Transforms the QA output from langchain into a dictionary format without the 'qa_pairs' field.\n",
    "    \n",
    "    Parameters:\n",
    "    - qa_output: A list of dictionaries, where each dictionary contains 'qa_pairs' among other possible fields.\n",
    "\n",
    "    Returns:\n",
    "    - A list of dictionaries, where each dictionary directly contains 'query' and 'answer' fields.\n",
    "    \"\"\"\n",
    "    parsed_output = []\n",
    "    for item in qa_output:\n",
    "        # Assuming each item in qa_output is a dictionary with a 'qa_pairs' key\n",
    "        qa_pair = item.get('qa_pairs', {})\n",
    "        # Repackage the qa_pair without the 'qa_pairs' field\n",
    "        reformatted_item = {\n",
    "            'query': qa_pair.get('query', ''),\n",
    "            'answer': qa_pair.get('answer', '')\n",
    "        }\n",
    "        parsed_output.append(reformatted_item)\n",
    "    return parsed_output\n",
    "\n",
    "\n",
    "def generate_qas(file_path, db, llm, chain_type):\n",
    "    # Load vector db to index\n",
    "    loader = CSVLoader(file_path=file_path)\n",
    "    data = loader.load()\n",
    "    index = VectorStoreIndexWrapper(vectorstore=db)\n",
    "\n",
    "    qa = RetrievalQA.from_chain_type(\n",
    "        llm=llm, \n",
    "        chain_type=chain_type, \n",
    "        retriever=index.vectorstore.as_retriever(), \n",
    "        verbose=True,\n",
    "        # chain_type_kwargs = {\n",
    "        #     \"document_separator\": \"<<<<>>>>>\"\n",
    "        # }\n",
    "    ) \n",
    "\n",
    "    # LLM-Generated example Q&A pairs \n",
    "    example_gen_chain = QAGenerateChain.from_llm(ChatOpenAI(model=llm_model()))\n",
    "    # the warning below can be safely ignored\n",
    "    raw_examples = example_gen_chain.apply( # create raw examples\n",
    "        [{\"doc\": t} for t in data[:5]],\n",
    "    )\n",
    "\n",
    "    # Parse the raw examples into required format\n",
    "    examples = langchain_output_parser(raw_examples)\n",
    "\n",
    "    # run for manual evaluation\n",
    "    qa.run(examples[0][\"query\"])\n",
    "\n",
    "    return qa, examples\n",
    "\n",
    "def evaluate(chain_type, qa, examples, llm, results_data):\n",
    "    # LLM assisted evaluation\n",
    "\n",
    "    # Measure number of tokens used\n",
    "    #TODO: remove duplication\n",
    "    with get_openai_callback() as cb:\n",
    "        start = datetime.now()\n",
    "\n",
    "        try:\n",
    "            predictions = qa.apply(examples)\n",
    "        except ValueError as e: \n",
    "            response = e\n",
    "\n",
    "        end = datetime.now()\n",
    "        \n",
    "    # Calculate the difference between the end and start timestamps to get the execution duration.\n",
    "    # The duration is converted to milliseconds for a more precise and readable format.\n",
    "    td = (end - start).total_seconds() * 10**3\n",
    "    tokens_used = cb.total_tokens\n",
    "\n",
    "    eval_chain = QAEvalChain.from_llm(llm)\n",
    "    graded_outputs = eval_chain.evaluate(examples, predictions)\n",
    "\n",
    "    # turn to object and return\n",
    "    # using llm as real answer and predicted answer are not similar in a string match sense, e.g. look at example_llm_eval.txt\n",
    "    for i, eg in enumerate(examples):\n",
    "        \n",
    "        example_number = i\n",
    "        query = predictions[i]['query']\n",
    "        answer = predictions[i]['answer']\n",
    "        predicted_answer = predictions[i]['result']\n",
    "        result = graded_outputs[i]['results']\n",
    "        \n",
    "        print(f\"Example {example_number}:\")\n",
    "        print(\"Question: \" + query)\n",
    "        print(\"Real Answer: \" + answer)\n",
    "        print(\"Predicted Answer: \" + predicted_answer)\n",
    "        print(\"Predicted Grade: \" + result)\n",
    "        print()\n",
    "\n",
    "        results_data = add_to_results_list(results_data, chain_type, query, time=td, tokens_used=tokens_used, example_number=i, predicted_answer=predicted_answer, answer=answer, result=result)\n",
    "    return results_data\n",
    "\n",
    "def add_to_results_list(results_data, chain_type, query, time=None, tokens_used=None, example_number=None, answer=None, predicted_answer=None, result=None):\n",
    "    found = False\n",
    "    for item in results_data:\n",
    "        if item.chain_type == chain_type:\n",
    "            # Update the existing dictionary\n",
    "            item.append_evaluation(time=time, tokens_used=tokens_used, example_number=example_number, \n",
    "                         predicted_query=query, answer=answer, predicted_answer=predicted_answer, result=result)\n",
    "            found = True\n",
    "            break\n",
    "\n",
    "    if not found:\n",
    "        # Append a new instance of ResultsData if no matching chain_type was found\n",
    "        results_data.append(ResultsData(chain_type=chain_type, time=time, tokens_used=tokens_used, \n",
    "                                        example_number=example_number, predicted_query=query, \n",
    "                                        answer=answer, predicted_answer=predicted_answer, \n",
    "                                        result=result))\n",
    "        \n",
    "    return results_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "def qa_analysis(llm, chain_type, retriever, verbose, query, number, results_data):\n",
    "    \"\"\"\n",
    "    Initializes a QA analysis with a given language model, chain type, and retriever.\n",
    "    Then, it runs the QA analysis, timing its execution and printing the response along with the execution time.\n",
    "    \"\"\"\n",
    "    # Initialize the RetrievalQA object with the specified parameters.\n",
    "    qa = RetrievalQA.from_chain_type(\n",
    "        llm=llm, \n",
    "        chain_type=chain_type, \n",
    "        retriever=retriever, \n",
    "        verbose=verbose\n",
    "    )\n",
    "\n",
    "    # Measure number of tokens used\n",
    "    with get_openai_callback() as cb:\n",
    "        start = datetime.datetime.now()\n",
    "\n",
    "        try:\n",
    "            # Execute the QA analysis\n",
    "            response = qa.invoke(query) #TODO: i've only added queries, no answers...\n",
    "        except ValueError as e: \n",
    "            response = e\n",
    "\n",
    "        end = datetime.datetime.now()\n",
    "    \n",
    "    tokens_used = cb.total_tokens\n",
    "\n",
    "    # Calculate the difference between the end and start timestamps to get the execution duration.\n",
    "    # The duration is converted to milliseconds for a more precise and readable format.\n",
    "    td = (end - start).total_seconds() * 10**3\n",
    "    \n",
    "    print(f\"Response: {response}\\nThe time of execution of above program is : {td:.03f}ms\")\n",
    "\n",
    "    results_data = add_to_results_list(results_data, chain_type, query, td, tokens_used, number, response)\n",
    "\n",
    "    print(\"\\n\\nTESTING\\n:\" + '\\n'.join([str(item) for item in results_data]))\n",
    "\n",
    "    return results_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def results_data_to_markdown_table(results_data_list):\n",
    "    headers = [\"Chain Type\", \"Eval Time\", \"Tokens Used\", \"Example Number\", \"Predicted Query\", \"Predicted Answer\", \"Answer\", \"Result\"]\n",
    "    markdown_table = \"| \" + \" | \".join(headers) + \" |\\n\"\n",
    "    markdown_table += \"| \" + \" | \".join([\"---\"] * len(headers)) + \" |\\n\"\n",
    "\n",
    "    for data in results_data_list:\n",
    "        for eval in data.eval:\n",
    "            # Ensure every value is a string, handling None and ensuring dict values are properly formatted or avoided\n",
    "            row = [\n",
    "                data.chain_type,\n",
    "                str(eval.get(\"time\", \"\")),\n",
    "                str(eval.get(\"tokens_used\", \"\")),\n",
    "                str(eval.get(\"example_number\", \"\")),\n",
    "                eval.get(\"query\", \"\"),\n",
    "                eval.get(\"predicted_answer\", \"\") if eval.get(\"predicted_answer\") is not None else \"\",\n",
    "                eval.get(\"answer\", \"\"),\n",
    "                eval.get(\"result\", \"\") if eval.get(\"result\") is not None else \"\"\n",
    "            ]\n",
    "            markdown_table += \"| \" + \" | \".join([str(item) for item in row]) + \" |\\n\"\n",
    "    \n",
    "    return markdown_table\n",
    "\n",
    "\n",
    "def write_markdown_table_to_file(markdown_table, filename):\n",
    "    # Write the markdown table to the specified file\n",
    "    with open(filename, 'w', encoding='utf-8') as file:\n",
    "        file.write(markdown_table)\n",
    "    \n",
    "    print(f\"Markdown table successfully written to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing vector DB from ../data/OutdoorClothingCatalog_1000.vecdb\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Example 0:\n",
      "Question: What is the weight of the Women's Campside Oxfords per pair?\n",
      "Real Answer: The Women's Campside Oxfords have an approximate weight of 1 lb. 1 oz. per pair.\n",
      "Predicted Answer: The Women's Campside Oxfords weigh approximately 1 lb. 1 oz. per pair.\n",
      "Predicted Grade: CORRECT\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "add_to_results_list() got an unexpected keyword argument 'predicted_answer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 39\u001b[0m\n\u001b[1;32m     36\u001b[0m examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Evaluate \u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m results_data \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstrat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqa\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresults_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[62], line 106\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(chain_type, qa, examples, llm, results_data)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted Grade: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m result)\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28mprint\u001b[39m()\n\u001b[0;32m--> 106\u001b[0m     results_data \u001b[38;5;241m=\u001b[39m \u001b[43madd_to_results_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchain_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokens_used\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokens_used\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_number\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredicted_answer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpredicted_answer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manswer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results_data\n",
      "\u001b[0;31mTypeError\u001b[0m: add_to_results_list() got an unexpected keyword argument 'predicted_answer'"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "    # Basic Setup\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "results_data = []\n",
    "strategies = [\"stuff\", \"map_reduce\", \"refine\", \"map_rerank\"]\n",
    "\n",
    "# Load data into vector db or use existing one\n",
    "file_path = '../data/OutdoorClothingCatalog_1000.csv'\n",
    "embedding = OpenAIEmbeddings()  # Define embedding\n",
    "\n",
    "# Check if vector DB exists for the CSV, and load or create accordingly\n",
    "db = check_and_load_vector_db(file_path, embedding)\n",
    "\n",
    "queries = [\"Please suggest a shirt with sunblocking\", \"Please suggest a shirt with sunblocking and tell me why this one\", \"Please suggest three shirts with sunblocking and tell me why. Give this back to me in markdown code as a table\", \"Please suggest three shirts with sunblocking and tell me why. Give this back to me in markdown code as a table, with a summary below outlining why sunblocking is important\"]\n",
    "\n",
    "# Configure LLM for querying\n",
    "# layers vector db on llm to inform decisions and responses\n",
    "llm = ChatOpenAI(temperature = 0.0, model=llm_model())\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "# Manual analysis - TODO: add answers\n",
    "# for index, query in enumerate(queries, start=1):\n",
    "#     results_data = qa_analysis(llm, \"stuff\", retriever, True, query, index, results_data)\n",
    "#     results_data = qa_analysis(llm, \"map_reduce\", retriever, True, query, index, results_data)\n",
    "#     results_data = qa_analysis(llm, \"refine\", retriever, True, query, index, results_data)\n",
    "#     results_data = qa_analysis(llm, \"map_rerank\", retriever, True, query, index, results_data)\n",
    "\n",
    "# LLM QA Gen AND Evaluate\n",
    "for strat in strategies:\n",
    "    # Generate evaluation Q&As\n",
    "    tuple = generate_qas(file_path, db, llm, strat)\n",
    "    qa = tuple[0]\n",
    "    examples = tuple[1]\n",
    "\n",
    "    # Evaluate \n",
    "    results_data = evaluate(strat, qa, examples, llm, results_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Generate results in markdown\n",
    "md_table = results_data_to_markdown_table(results_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Markdown table successfully written to results.md\n"
     ]
    }
   ],
   "source": [
    "    # Write results to file\n",
    "write_markdown_table_to_file(md_table, \"results.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/langchain/chains/llm.py:344: UserWarning: The apply_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/langchain/chains/llm.py:344: UserWarning: The apply_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/langchain/chains/llm.py:344: UserWarning: The apply_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/langchain/chains/llm.py:344: UserWarning: The apply_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/langchain/chains/llm.py:344: UserWarning: The apply_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "[{'query': \"What is the approximate weight of the Women's Campside Oxfords per pair?\", 'answer': \"The approximate weight of the Women's Campside Oxfords per pair is 1 lb.1 oz.\", 'result': 'Approx. weight: 1 lb.1 oz. per pair.'}, {'query': 'What materials are used to construct the Recycled Waterhog dog mat?', 'answer': 'The Recycled Waterhog dog mat is constructed from 24 oz. polyester fabric made from 94% recycled materials and has a rubber backing.', 'result': 'The Recycled Waterhog dog mat is constructed from 24 oz. polyester fabric made from 94% recycled materials and has a rubber backing.'}, {'query': \"What features does the Infant and Toddler Girls' Coastal Chill Swimsuit have?\", 'answer': \"The swimsuit has bright colors, ruffles, and exclusive whimsical prints. It is made of four-way-stretch and chlorine-resistant fabric, which keeps its shape and resists snags. The fabric is also UPF 50+ rated, providing the highest rated sun protection possible, blocking 98% of the sun's harmful rays. The swimsuit has crossover no-slip straps and a fully lined bottom, ensuring a secure fit and maximum coverage. It can be machine washed and line dried for best results.\", 'result': \"The Infant and Toddler Girls' Coastal Chill Swimsuit has bright colors, ruffles, exclusive whimsical prints, four-way-stretch and chlorine-resistant fabric, UPF 50+ rated fabric, crossover no-slip straps, fully lined bottom, and is machine washable and line dryable.\"}, {'query': 'What is the fabric composition of the Refresh Swimwear, V-Neck Tankini Contrasts?', 'answer': 'The body of the swimwear is made of 82% recycled nylon and 18% Lycra® spandex, while the lining is made of 90% recycled nylon and 10% Lycra® spandex.', 'result': 'Body in 82% recycled nylon with 18% Lycra® spandex. Lined in 90% recycled nylon with 10% Lycra® spandex.'}, {'query': 'What is the primary feature of the EcoFlex 3L Storm Pants that sets them apart from other waterproof pants?', 'answer': 'The primary feature that sets the EcoFlex 3L Storm Pants apart from other waterproof pants is the state-of-the-art TEK O2 technology that offers the most breathability the company has ever tested.', 'result': 'The primary feature of the EcoFlex 3L Storm Pants that sets them apart from other waterproof pants is the state-of-the-art TEK O2 technology that offers the most breathability ever tested. '}]\n"
     ]
    }
   ],
   "source": [
    "predictions = qa.apply(examples)\n",
    "\n",
    "print(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
