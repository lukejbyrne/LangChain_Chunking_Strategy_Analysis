Response: verbose=True combine_documents_chain=StuffDocumentsChain(llm_chain=LLMChain(prompt=ChatPromptTemplate(input_variables=['context', 'question'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], template="Use the following pieces of context to answer the user's question. \nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n{context}")), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], template='{question}'))]), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x16917d8d0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x16916f710>, model_name='gpt-3.5-turbo-0301', temperature=0.0, openai_api_key='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', openai_proxy='')), document_variable_name='context') retriever=VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x1262ee950>)
The time of execution of above program is : 0.481ms
Response: verbose=True combine_documents_chain=MapReduceDocumentsChain(llm_chain=LLMChain(prompt=ChatPromptTemplate(input_variables=['context', 'question'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], template='Use the following portion of a long document to see if any of the text is relevant to answer the question. \nReturn any relevant text verbatim.\n______________________\n{context}')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], template='{question}'))]), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x16917d8d0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x16916f710>, model_name='gpt-3.5-turbo-0301', temperature=0.0, openai_api_key='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', openai_proxy='')), reduce_documents_chain=ReduceDocumentsChain(combine_documents_chain=StuffDocumentsChain(llm_chain=LLMChain(prompt=ChatPromptTemplate(input_variables=['question', 'summaries'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['summaries'], template="Given the following extracted parts of a long document and a question, create a final answer. \nIf you don't know the answer, just say that you don't know. Don't try to make up an answer.\n______________________\n{summaries}")), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], template='{question}'))]), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x16917d8d0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x16916f710>, model_name='gpt-3.5-turbo-0301', temperature=0.0, openai_api_key='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', openai_proxy='')), document_variable_name='summaries')), document_variable_name='context') retriever=VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x1262ee950>)
The time of execution of above program is : 0.197ms
Response: verbose=True combine_documents_chain=RefineDocumentsChain(initial_llm_chain=LLMChain(prompt=ChatPromptTemplate(input_variables=['context_str', 'question'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context_str'], template='Context information is below.\n------------\n{context_str}\n------------\nGiven the context information and not prior knowledge, answer any questions')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], template='{question}'))]), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x16917d8d0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x16916f710>, model_name='gpt-3.5-turbo-0301', temperature=0.0, openai_api_key='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', openai_proxy='')), refine_llm_chain=LLMChain(prompt=ChatPromptTemplate(input_variables=['context_str', 'existing_answer', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], template='{question}')), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=['existing_answer'], template='{existing_answer}')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context_str'], template="We have the opportunity to refine the existing answer (only if needed) with some more context below.\n------------\n{context_str}\n------------\nGiven the new context, refine the original answer to better answer the question. If the context isn't useful, return the original answer."))]), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x16917d8d0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x16916f710>, model_name='gpt-3.5-turbo-0301', temperature=0.0, openai_api_key='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', openai_proxy='')), document_variable_name='context_str', initial_response_name='existing_answer') retriever=VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x1262ee950>)
The time of execution of above program is : 0.245ms
Response: verbose=True combine_documents_chain=MapRerankDocumentsChain(llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['context', 'question'], output_parser=RegexParser(regex='(.*?)\\nScore: (\\d*)', output_keys=['answer', 'score']), template="Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n\nIn addition to giving an answer, also return a score of how fully it answered the user's question. This should be in the following format:\n\nQuestion: [question here]\nHelpful Answer: [answer here]\nScore: [score between 0 and 100]\n\nHow to determine the score:\n- Higher is a better answer\n- Better responds fully to the asked question, with sufficient level of detail\n- If you do not know the answer based on the context, that should be a score of 0\n- Don't be overconfident!\n\nExample #1\n\nContext:\n---------\nApples are red\n---------\nQuestion: what color are apples?\nHelpful Answer: red\nScore: 100\n\nExample #2\n\nContext:\n---------\nit was night and the witness forgot his glasses. he was not sure if it was a sports car or an suv\n---------\nQuestion: what type was the car?\nHelpful Answer: a sports car or an suv\nScore: 60\n\nExample #3\n\nContext:\n---------\nPears are either red or orange\n---------\nQuestion: what color are apples?\nHelpful Answer: This document does not answer the question\nScore: 0\n\nBegin!\n\nContext:\n---------\n{context}\n---------\nQuestion: {question}\nHelpful Answer:"), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x16917d8d0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x16916f710>, model_name='gpt-3.5-turbo-0301', temperature=0.0, openai_api_key='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', openai_proxy='')), document_variable_name='context', rank_key='score', answer_key='answer') retriever=VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x1262ee950>)
The time of execution of above program is : 0.097ms